\expandafter\ifx\csname doTocEntry\endcsname\relax \expandafter\endinput\fi 
\doTocEntry\tocsection{1}{\csname a:TocLink\endcsname{1}{x1-10001}{QQ2-1-1}{Introduction}}{3}\relax 
\doTocEntry\tocsection{2}{\csname a:TocLink\endcsname{1}{x1-20002}{QQ2-1-2}{Neural network}}{4}\relax 
\doTocEntry\tocsubsection{2.1}{\csname a:TocLink\endcsname{1}{x1-30002.1}{QQ2-1-3}{ Node (neuron or unit), weight, bias, and activation}}{4}\relax 
\doTocEntry\toclof{1}{\csname a:TocLink\endcsname{1}{x1-30011}{}{\ignorespaces Definition of layers, neurons, weights, and biases in a neural network. The $j^{\ensuremath  {\operatorname  {th}}}$ neuron in the $l^{\ensuremath  {\operatorname  {th}}}$ layer is referred to as neuron $(l, j)$}}{figure}\relax 
\doTocEntry\tocsubsection{2.2}{\csname a:TocLink\endcsname{1}{x1-40002.2}{QQ2-1-5}{Objective function}}{9}\relax 
\doTocEntry\tocsubsection{2.3}{\csname a:TocLink\endcsname{1}{x1-50002.3}{QQ2-1-6}{Gradients of objective function}}{11}\relax 
\doTocEntry\tocsubsection{2.4}{\csname a:TocLink\endcsname{1}{x1-60002.4}{QQ2-1-7}{Back-propagating algorithm}}{13}\relax 
\doTocEntry\tocsection{3}{\csname a:TocLink\endcsname{1}{x1-70003}{QQ2-1-8}{misc}}{17}\relax 
\doTocEntry\tocsection{4}{\csname a:TocLink\endcsname{1}{x1-80004}{QQ2-1-9}{Least square}}{17}\relax 
\doTocEntry\tocsection{5}{\csname a:TocLink\endcsname{1}{x1-90005}{QQ2-1-10}{Logistic regression for binary classification}}{19}\relax 
\doTocEntry\tocsubsection{5.1}{\csname a:TocLink\endcsname{1}{x1-100005.1}{QQ2-1-11}{Automatic differentiation}}{22}\relax 
\doTocEntry\toclikesection{}{\csname a:TocLink\endcsname{1}{x1-110005.1}{QQ2-1-12}{References}}{24}\relax 
\par 
