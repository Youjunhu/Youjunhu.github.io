<?xml version="1.0" encoding="utf-8" ?> 
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" 
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">  
<!--http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd-->  
<html xmlns="http://www.w3.org/1999/xhtml"  
> 
<head>
   <title>Machine learning</title> 
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" /> 
<meta name="generator" content="TeX4ht (http://www.tug.org/tex4ht/)" /> 
<meta name="originator" content="TeX4ht (http://www.tug.org/tex4ht/)" /> 
<!-- xhtml,1,charset=utf-8,html --> 
<meta name="src" content="machine_learning.tex" /> 
<link rel="stylesheet" type="text/css" href="machine_learning.css" /> 
</head><body 
>
   <div class="maketitle">



<h2 class="titleHead">Machine learning</h2>
  <div class="author" > <span 
class="cmr-12">Youjun Hu</span>
<br /><span 
class="cmr-12">yjhu@ipp.cas.cn </span></div><br />
<div class="date" ><span 
class="cmr-12">June 28, 2024</span></div>
   </div>
<!--l. 24--><p class="indent" >    
</p>
   <div class="tableofcontents">
   <span class="sectionToc" >1 <a 
href="#x1-10001" id="QQ2-1-1">Introduction</a></span>
<br />   <span class="sectionToc" >2 <a 
href="#x1-20002" id="QQ2-1-2">Neural network</a></span>
<br />   <span class="sectionToc" >3 <a 
href="#x1-70003" id="QQ2-1-8">misc</a></span>
<br />   <span class="sectionToc" >4 <a 
href="#x1-80004" id="QQ2-1-9">Least square</a></span>
<br />   <span class="sectionToc" >5 <a 
href="#x1-90005" id="QQ2-1-10">Logistic regression for binary classiﬁcation</a></span>
<br />   <span class="likesectionToc" ><a 
href="#x1-110005.1" id="QQ2-1-12">References</a></span>
   </div>
   <h3 class="sectionHead"><span class="titlemark">1   </span> <a 
 id="x1-10001"></a>Introduction</h3>
<!--l. 28--><p class="noindent" >Artiﬁcial intelligence (AI) research has tried many diﬀerent approaches since its founding. In the ﬁrst
decades of the 21st century, the AI research is dominated by highly mathematical optimization
machine learning (ML), which has proved successful in solving many challenging real life
problems.
</p><!--l. 33--><p class="indent" >   Many problems in AI can be solved theoretically by searching through many possible
solutions: Reasoning can be reduced to performing a search. Simple exhaustive searches
are rarely suﬃcient for most real-world problems. The solution, for many problems, is to
use ”heuristics” or ”rules of thumb” that prioritize choices in favor of those more likely to
reach a goal. A very diﬀerent kind of search came to prominence in the 1990s, based on the
mathematical theory of optimization. Modern machine learning is based on these methods.
Instead, of using detailed explanations to guide the search, it uses a combination of<span class="cite">[<a 
href="#Xnielsen2015neural">1</a>]</span>: (a)
general architectures; (b) trying trillions of possibilities, guided by simple ideas (like gradient
descent) for improvement; and (c) the ability to recognize progress (by deﬁning a objective
function).
</p><!--l. 46--><p class="indent" >   I am interested in applying machine learning to problems in computational physics problems that
traditional numerical methods can not easily handle either because of its computational costs being too
high or its traditional algorithms are too complicated to easily implement.

</p><!--l. 51--><p class="indent" >   Enrico Fermi once criticized the complexity of a model (that contains many free parameters) by
quoting Johnny von Neumann “With four parameters I can ﬁt an elephant, and with ﬁve I can make
him wiggle his trunk”. What Fermi implies is that it is easy to ﬁt existing data and what is important
is to have a model with predicting capability (ﬁtting data not seen yet). The artiﬁcial neural network
method tackles this diﬃculty by increasing the number of free parameters to millions, with the hope of
obtaining predicting capability.
</p><!--l. 60--><p class="noindent" >
</p>
   <h3 class="sectionHead"><span class="titlemark">2   </span> <a 
 id="x1-20002"></a>Neural network</h3>
<!--l. 62--><p class="noindent" >Neural networks consists of multiple layers of interconnected nodes (neurons), each having a weight for
a connection, a bias, and an activation function. Each layer build upon the previous layer. This
progression of computations through the network is called forward propagation. Another process called
backpropagation uses algorithms which moves backwards through the layers to eﬃciently compute the
partial derivatives of the objective function with respect to the weights and biases. Combining
the forward and backward propagation, we can calculate errors in predictions and then
adjusts the weights and biases using the gradient descent method. This process is called
training/learning.
</p><!--l. 73--><p class="noindent" >
</p>
   <h4 class="subsectionHead"><span class="titlemark">2.1   </span> <a 
 id="x1-30002.1"></a> Node (neuron or unit), weight, bias, and activation</h4>
<!--l. 75--><p class="noindent" >As is shown in Fig. <a 
href="#x1-30011">1<!--tex4ht:ref: 22-3-13-a1 --></a>, we use <span 
class="cmmi-10">w</span><sub><span 
class="cmmi-7">jk</span></sub><sup><span 
class="cmmi-7">l</span></sup> to denote the weight for the connection from the <span 
class="cmmi-10">k</span><sup><span 
class="cmr-7">th</span></sup> neuron in the
(<span 
class="cmmi-10">l </span><span 
class="cmsy-10">− </span>1)<sup><span 
class="cmr-7">th</span></sup> layer to the <span 
class="cmmi-10">j</span><sup><span 
class="cmr-7">th</span></sup> neuron in the <span 
class="cmmi-10">l</span><sup><span 
class="cmr-7">th</span></sup> layer. Use <span 
class="cmmi-10">b</span><sub><span 
class="cmmi-7">j</span></sub><sup><span 
class="cmmi-7">l</span></sup> to denote the bias of the <span 
class="cmmi-10">j</span><sup><span 
class="cmr-7">th</span></sup> neuron in the <span 
class="cmmi-10">l</span><sup><span 
class="cmr-7">th</span></sup>
layer.
</p>
   <hr class="figure" /><div class="figure" 
>

<a 
 id="x1-30011"></a>

<!--l. 82--><p class="noindent" > <img 
src="machine_learning-1-.png" alt="PIC"  
width="266" height="170"  />
<br /> </p><div class="caption" 
><span class="id">Figure 1: </span><span  
class="content">Deﬁnition of layers, neurons, weights, and biases in a neural network. The <span 
class="cmmi-10">j</span><sup><span 
class="cmr-7">th</span></sup> neuron
in the <span 
class="cmmi-10">l</span><sup><span 
class="cmr-7">th</span></sup> layer is referred to as neuron (<span 
class="cmmi-10">l,j</span>)</span></div><!--tex4ht:label?: x1-30011 -->

   </div><hr class="endfigure" />
<!--l. 88--><p class="indent" >    
</p><!--l. 90--><p class="indent" >   We use <span 
class="cmmi-10">a</span><sub><span 
class="cmmi-7">j</span></sub><sup><span 
class="cmmi-7">l</span></sup> to denote the output (activation) of the <span 
class="cmmi-10">j</span><sup><span 
class="cmr-7">th</span></sup> neuron in <span 
class="cmmi-10">l</span><sup><span 
class="cmr-7">th</span></sup> layer. A neural network model
assumes that <span 
class="cmmi-10">a</span><sub><span 
class="cmmi-7">j</span></sub><sup><span 
class="cmmi-7">l</span></sup> is related to the <span 
class="cmmi-10">a</span><sup><span 
class="cmmi-7">l</span><span 
class="cmsy-7">−</span><span 
class="cmr-7">1</span></sup> (output of the previous layer) via
</p>
   <table 
class="equation"><tr><td><a 
 id="x1-3002r1"></a>
   <center class="math-display" >
<img 
src="machine_learning0x.png" alt="      (              )
 l     ∑    l l−1   l
aj = σ    w jkak  + bj  ,
        k
" class="math-display"  /></center></td><td class="equation-label">(1)</td></tr></table>
<!--l. 96--><p class="nopar" >
where the summation is over all neurons in the (<span 
class="cmmi-10">l </span><span 
class="cmsy-10">− </span>1)<sup><span 
class="cmr-7">th</span></sup> layer and <span 
class="cmmi-10">σ </span>is a function called activation
function which can take various forms, e.g., step function,
</p>
   <table 
class="equation"><tr><td><a 
 id="x1-3003r2"></a>
   <center class="math-display" >
<img 
src="machine_learning1x.png" alt="      { 1ifz ≥ 0
σ(z) =  0  else  ,
" class="math-display"  /></center></td><td class="equation-label">(2)</td></tr></table>
<!--l. 105--><p class="nopar" >
rectiﬁed linear unit (ReLU),
</p>
   <table 
class="equation"><tr><td><a 
 id="x1-3004r3"></a>
   <center class="math-display" >
<img 
src="machine_learning2x.png" alt="σ (z) = max(0,z),
" class="math-display"  /></center></td><td class="equation-label">(3)</td></tr></table>
<!--l. 109--><p class="nopar" >
and sigmoid (“S”-shaped curve, also called logistic function)
</p>
   <table 
class="equation"><tr><td><a 
 id="x1-3005r4"></a>
   <center class="math-display" >
<img 
src="machine_learning3x.png" alt="σ(z) =-----1-----.
      1 + exp (− z)
" class="math-display"  /></center></td><td class="equation-label">(4)</td></tr></table>
<!--l. 113--><p class="nopar" >
For natation ease, deﬁne <span 
class="cmmi-10">z</span><sub><span 
class="cmmi-7">j</span></sub><sup><span 
class="cmmi-7">l</span></sup> by
</p>
   <table 
class="equation"><tr><td><a 
 id="x1-3006r5"></a>
   <center class="math-display" >
<img 
src="machine_learning4x.png" alt="     ∑
zlj =    wljkal−k1 + blj,
      k
" class="math-display"  /></center></td><td class="equation-label">(5)</td></tr></table>
<!--l. 117--><p class="nopar" >
which can be interpreted as an weighted input to the neuron (<span 
class="cmmi-10">l,j</span>), then Eq. (<a 
href="#x1-3002r1">1<!--tex4ht:ref: 22-3-9-e2 --></a>) is written
as
</p>
   <table 
class="equation"><tr><td><a 
 id="x1-3007r6"></a>
   <center class="math-display" >
<img 
src="machine_learning5x.png" alt="alj = σ (zlj).
" class="math-display"  /></center></td><td class="equation-label">(6)</td></tr></table>
<!--l. 122--><p class="nopar" >
In matrix form, Eq. (<a 
href="#x1-3006r5">5<!--tex4ht:ref: 22-3-9-1 --></a>) is written as
</p>
   <table 
class="equation"><tr><td><a 
 id="x1-3008r7"></a>
   <center class="math-display" >
<img 
src="machine_learning6x.png" alt="zl = wlal−1 + bl,
" class="math-display"  /></center></td><td class="equation-label">(7)</td></tr></table>
<!--l. 126--><p class="nopar" >
where <span 
class="cmmi-10">w</span><sup><span 
class="cmmi-7">l</span></sup> is a <span 
class="cmmi-10">J </span><span 
class="cmsy-10">× </span><span 
class="cmmi-10">K </span>matrix, <span 
class="cmmi-10">z</span><sup><span 
class="cmmi-7">l</span></sup>  and <span 
class="cmmi-10">b</span><sup><span 
class="cmmi-7">l</span></sup> are column vectors of length <span 
class="cmmi-10">J</span>, <span 
class="cmmi-10">a</span><sup><span 
class="cmmi-7">l</span><span 
class="cmsy-7">−</span><span 
class="cmr-7">1</span></sup> is a column vector of
length <span 
class="cmmi-10">K</span>, where <span 
class="cmmi-10">J </span>and <span 
class="cmmi-10">K </span>are the number of neurons in the <span 
class="cmmi-10">l</span><sup><span 
class="cmr-7">th</span></sup> layer and (<span 
class="cmmi-10">l </span><span 
class="cmsy-10">− </span>1)<sup><span 
class="cmr-7">th</span></sup> layer,
respectively.
</p><!--l. 132--><p class="indent" >   The input layer is where data inputs are provided, and the output layer is where the ﬁnal prediction
is made. The input and output layers of a deep neural network are called visible layers. The layers
between the input layer and output layer are called hidden layers. Note that the input layer is
usually not considered as a layer of the network since it does not involve any computation. In
tensorﬂow, layers refer to the computing layers (i.e., hidden layers and the output layer,
not including the input layer). The activation function of each layer can be diﬀerent. The
activation function of the output layer is often chosen as None, ReLU, logistic/tanh, and
is usually diﬀerent from those used in the hidden layers. Here “None” means activation
<span 
class="cmmi-10">σ</span>(<span 
class="cmmi-10">z</span>) = <span 
class="cmmi-10">z</span>.
</p>
   <h4 class="subsectionHead"><span class="titlemark">2.2   </span> <a 
 id="x1-40002.2"></a>Objective function</h4>
<!--l. 146--><p class="noindent" >Deﬁne an objective function (can be called loss or cost function depending on contexts)
by
</p>
   <table 
class="equation"><tr><td><a 
 id="x1-4001r8"></a>
   <center class="math-display" >
<img 
src="machine_learning7x.png" alt="C(w,b) ≡ 1-∑  ∥y(x)− aL∥2,
         2n  x
" class="math-display"  /></center></td><td class="equation-label">(8)</td></tr></table>
<!--l. 150--><p class="nopar" >
where <span 
class="cmmi-10">w </span>and <span 
class="cmmi-10">b </span>denotes the collection of all weights and biases in the network, <span 
class="cmmi-10">n </span>is the total number of
training examples <span 
class="cmmi-10">x</span>, the summation is over all the training examples, <span 
class="cmmi-10">y</span>(<span 
class="cmmi-10">x</span>) is the desired output from
the network (i.e., correct answer) when <span 
class="cmmi-10">x </span>is the input, and <span 
class="cmmi-10">a</span><sup><span 
class="cmmi-7">L</span></sup> is the actual output from
the output layer of the network and is a function of <span 
class="cmmi-10">w,b</span>, and <span 
class="cmmi-10">x</span>. Note that <span 
class="cmmi-10">y </span>and <span 
class="cmmi-10">a</span><sup><span 
class="cmmi-7">L</span></sup> are
vectors (with number of elements being the number of neurons in the output layer) and
<span 
class="cmsy-10">∥</span><span 
class="cmmi-10">…</span><span 
class="cmsy-10">∥ </span>denotes the vector norm. Explicitly writing out the vector norm, Eq. (<a 
href="#x1-4001r8">8<!--tex4ht:ref: 22-3-9-e1 --></a>) is written
as
</p>
   <table 
class="equation"><tr><td><a 
 id="x1-4002r9"></a>
   <center class="math-display" >
<img 
src="machine_learning8x.png" alt="          1 ∑  N∑L
C (w, b) ≡ 2n-     (yj(x)− aLj )2,
             x j=1
" class="math-display"  /></center></td><td class="equation-label">(9)</td></tr></table>
<!--l. 162--><p class="nopar" >
where <span 
class="cmmi-10">N</span><sub><span 
class="cmmi-7">L</span></sub> is the number of neurons in the output layer.
</p><!--l. 165--><p class="indent" >   The cost function is the average error of the approximate solution away from the desired exact
solution. The goal of a learning algorithm is to ﬁnd weights and biases that minimize the
cost function. A method of minimizing the cost function over (<span 
class="cmmi-10">w,b</span>) is the gradient descent
method:
</p>
   <table 
class="equation"><tr><td><a 
 id="x1-4003r10"></a>
   <center class="math-display" >
<img 
src="machine_learning9x.png" alt="wljk → wljk − η-∂Cl-,
            ∂w jk
" class="math-display"  /></center></td><td class="equation-label">(10)</td></tr></table>
<!--l. 172--><p class="nopar" >

</p>
   <table 
class="equation"><tr><td><a 
 id="x1-4004r11"></a>
   <center class="math-display" >
<img 
src="machine_learning10x.png" alt="bl→  bl − η∂C-,
 j   j    ∂blj
" class="math-display"  /></center></td><td class="equation-label">(11)</td></tr></table>
<!--l. 176--><p class="nopar" >
where <span 
class="cmmi-10">η </span>is called learning rate, which should be positive.
</p><!--l. 179--><p class="indent" >   In using the gradient descent method, we need to compute the partial derivatives <span 
class="cmmi-10">∂C∕∂w</span><sub><span 
class="cmmi-7">jk</span></sub><sup><span 
class="cmmi-7">l</span></sup> and
<span 
class="cmmi-10">∂C∕∂b</span><sub><span 
class="cmmi-7">j</span></sub><sup><span 
class="cmmi-7">l</span></sup>. Next we will discuss how to compute them.
</p><!--l. 183--><p class="noindent" >
</p>
   <h4 class="subsectionHead"><span class="titlemark">2.3   </span> <a 
 id="x1-50002.3"></a>Gradients of objective function</h4>
<!--l. 185--><p class="noindent" >Note that the loss function involves an average over all the training examples. Denote the loss function
for a speciﬁc training example by <span 
class="cmmi-10">C</span><sub><span 
class="cmmi-7">x</span></sub>, i.e.,
</p>
   <table 
class="equation"><tr><td><a 
 id="x1-5001r12"></a>
   <center class="math-display" >
<img 
src="machine_learning11x.png" alt="       N∑L
Cx = 1   (yj(x)− aLj )2,
     2 j=1
" class="math-display"  /></center></td><td class="equation-label">(12)</td></tr></table>
<!--l. 190--><p class="nopar" >
then expression (<a 
href="#x1-4002r9">9<!--tex4ht:ref: 22-3-9-e1m --></a>) is written as
</p>
   <table 
class="equation"><tr><td><a 
 id="x1-5002r13"></a>

   <center class="math-display" >
<img 
src="machine_learning12x.png" alt="    1 ∑
C = --   Cx,
    n  x
" class="math-display"  /></center></td><td class="equation-label">(13)</td></tr></table>
<!--l. 194--><p class="nopar" >
Then the partial derivatives <span 
class="cmmi-10">∂C∕∂w</span><sub><span 
class="cmmi-7">jk</span></sub><sup><span 
class="cmmi-7">l</span></sup> and <span 
class="cmmi-10">∂C∕∂b</span><sub><span 
class="cmmi-7">j</span></sub><sup><span 
class="cmmi-7">l</span></sup> can be written as the sum of <span 
class="cmmi-10">∂C</span><sub><span 
class="cmmi-7">x</span></sub><span 
class="cmmi-10">∕∂w</span><sub><span 
class="cmmi-7">jk</span></sub><sup><span 
class="cmmi-7">l</span></sup> and
<span 
class="cmmi-10">∂C</span><sub><span 
class="cmmi-7">x</span></sub><span 
class="cmmi-10">∕∂b</span><sub><span 
class="cmmi-7">j</span></sub><sup><span 
class="cmmi-7">l</span></sup>, i.e.,
</p>
   <table 
class="equation"><tr><td><a 
 id="x1-5003r14"></a>
   <center class="math-display" >
<img 
src="machine_learning13x.png" alt=" ∂C    1 ∑   ∂C
---l-= --   ---xl-,
∂w jk   n  x ∂w jk
" class="math-display"  /></center></td><td class="equation-label">(14)</td></tr></table>
<!--l. 201--><p class="nopar" >
</p>
   <table 
class="equation"><tr><td><a 
 id="x1-5004r15"></a>
   <center class="math-display" >
<img 
src="machine_learning14x.png" alt="        ∑
∂Cl = 1-   ∂Cxl-.
∂bj   n  x ∂bj
" class="math-display"  /></center></td><td class="equation-label">(15)</td></tr></table>
<!--l. 206--><p class="nopar" >
The above formulas indicate that, once <span 
class="cmmi-10">∂C</span><sub><span 
class="cmmi-7">x</span></sub><span 
class="cmmi-10">∕∂w</span><sub><span 
class="cmmi-7">jk</span></sub><sup><span 
class="cmmi-7">l</span></sup> and <span 
class="cmmi-10">∂C</span><sub><span 
class="cmmi-7">x</span></sub><span 
class="cmmi-10">∕∂b</span><sub><span 
class="cmmi-7">j</span></sub><sup><span 
class="cmmi-7">l</span></sup> are known, obtaining  <span 
class="cmmi-10">∂C∕∂w</span><sub><span 
class="cmmi-7">jk</span></sub><sup><span 
class="cmmi-7">l</span></sup>
and <span 
class="cmmi-10">∂C∕∂b</span><sub><span 
class="cmmi-7">j</span></sub><sup><span 
class="cmmi-7">l</span></sup> is trivial, i.e., just averaging them. Therefore, we will focus on <span 
class="cmmi-10">C</span><sub><span 
class="cmmi-7">x</span></sub> (i.e., the
cost function for a ﬁxed training example) and discuss how to compute <span 
class="cmmi-10">∂C</span><sub><span 
class="cmmi-7">x</span></sub><span 
class="cmmi-10">∕∂w</span><sub><span 
class="cmmi-7">jk</span></sub><sup><span 
class="cmmi-7">l</span></sup> and
<span 
class="cmmi-10">∂C</span><sub><span 
class="cmmi-7">x</span></sub><span 
class="cmmi-10">∕∂b</span><sub><span 
class="cmmi-7">j</span></sub><sup><span 
class="cmmi-7">l</span></sup>.

</p><!--l. 214--><p class="indent" >   In practice, we do not sum over all the training examples. Instead, we average the derivative over a
small number (say 16) of training examples (a mini batch) and use these approximate derivatives to
advance a step. For the next step, we stochastically change to using a diﬀerent mini batch. This is
called stochastic gradient descent (SGD) method.
</p><!--l. 220--><p class="noindent" >
</p>
   <h4 class="subsectionHead"><span class="titlemark">2.4   </span> <a 
 id="x1-60002.4"></a>Back-propagating algorithm</h4>
<!--l. 222--><p class="noindent" >The cost function <span 
class="cmmi-10">C</span><sub><span 
class="cmmi-7">x</span></sub> is a function of weights and biases of all neurons (the input <span 
class="cmmi-10">x </span>and output
<span 
class="cmmi-10">y</span>(<span 
class="cmmi-10">x</span>) are ﬁxed parameters). For a speciﬁc neuron (<span 
class="cmmi-10">l,j</span>), its weights and biases enter <span 
class="cmmi-10">C</span><sub><span 
class="cmmi-7">x</span></sub> via
the combination <span 
class="cmmi-10">z</span><sub><span 
class="cmmi-7">j</span></sub><sup><span 
class="cmmi-7">l</span></sup> = <span 
class="cmex-10">∑</span>
  <sub><span 
class="cmmi-7">k</span></sub><span 
class="cmmi-10">w</span><sub><span 
class="cmmi-7">jk</span></sub><sup><span 
class="cmmi-7">l</span></sup><span 
class="cmmi-10">a</span><sub><span 
class="cmmi-7">k</span></sub><sup><span 
class="cmmi-7">l</span><span 
class="cmsy-7">−</span><span 
class="cmr-7">1</span></sup> + <span 
class="cmmi-10">b</span><sub><span 
class="cmmi-7">j</span></sub><sup><span 
class="cmmi-7">l</span></sup>. Then it is useful to deﬁne the following partial
derivative:
</p>
   <table 
class="equation"><tr><td><a 
 id="x1-6001r16"></a>
   <center class="math-display" >
<img 
src="machine_learning15x.png" alt=" l   ∂Cx
δj ≡ ∂zl,
       j
" class="math-display"  /></center></td><td class="equation-label">(16)</td></tr></table>
<!--l. 229--><p class="nopar" >
where the partial derivative are taken with ﬁxed weights and biases for all neurons except neuron (<span 
class="cmmi-10">l,j</span>).
Note that the <span 
class="cmmi-10">a</span><sub><span 
class="cmmi-7">k</span></sub><sup><span 
class="cmmi-7">l</span><span 
class="cmsy-7">−</span><span 
class="cmr-7">1</span></sup> appearing in the expression of <span 
class="cmmi-10">z</span><sub><span 
class="cmmi-7">j</span></sub><sup><span 
class="cmmi-7">l</span></sup> does not depend on <span 
class="cmmi-10">w</span><sub><span 
class="cmmi-7">jk</span></sub><sup><span 
class="cmmi-7">l</span></sup> or <span 
class="cmmi-10">b</span><sub><span 
class="cmmi-7">j</span></sub><sup><span 
class="cmmi-7">l</span></sup>. It only
depends on the weights and biases in the layers <span 
class="msam-10">≤ </span>(<span 
class="cmmi-10">l </span><span 
class="cmsy-10">− </span>1), which are all ﬁxed when taking the
derivative in expression (<a 
href="#x1-6001r16">16<!--tex4ht:ref: 22-3-11-p1 --></a>). <span 
class="cmmi-10">δ</span><sub><span 
class="cmmi-7">j</span></sub><sup><span 
class="cmmi-7">l</span></sup> deﬁned in expression (<a 
href="#x1-6001r16">16<!--tex4ht:ref: 22-3-11-p1 --></a>) is often called the error of neuron
(<span 
class="cmmi-10">l,j</span>).
</p><!--l. 238--><p class="indent" >   Using the chain rule,  <span 
class="cmmi-10">∂C</span><sub><span 
class="cmmi-7">x</span></sub><span 
class="cmmi-10">∕∂w</span><sub><span 
class="cmmi-7">jk</span></sub><sup><span 
class="cmmi-7">l</span></sup> and <span 
class="cmmi-10">∂C</span><sub><span 
class="cmmi-7">x</span></sub><span 
class="cmmi-10">∕∂b</span><sub><span 
class="cmmi-7">j</span></sub><sup><span 
class="cmmi-7">l</span></sup> can be expressed in terms of <span 
class="cmmi-10">δ</span><sub><span 
class="cmmi-7">j</span></sub><sup><span 
class="cmmi-7">l</span></sup>:
</p>
   <table 
class="equation"><tr><td><a 
 id="x1-6002r17"></a>
   <center class="math-display" >
<img 
src="machine_learning16x.png" alt="∂C    ∂C  ∂zl
--xl-= ---xl--jl = δjl,
∂bj   ∂zj ∂bj
" class="math-display"  /></center></td><td class="equation-label">(17)</td></tr></table>

<!--l. 243--><p class="nopar" >
and
</p>
   <table 
class="equation"><tr><td><a 
 id="x1-6003r18"></a>
   <center class="math-display" >
<img 
src="machine_learning17x.png" alt="∂Cx--  ∂Cx--∂zlj-   ll−1
∂wl  = ∂zl ∂wl  = δjak  .
  jk     j   jk
" class="math-display"  /></center></td><td class="equation-label">(18)</td></tr></table>
<!--l. 248--><p class="nopar" >
Therefore, if <span 
class="cmmi-10">δ</span><sub><span 
class="cmmi-7">j</span></sub><sup><span 
class="cmmi-7">l</span></sup> is known, it is trivial to compute the gradients needed in the gradient descent
method.
</p><!--l. 254--><p class="indent" >    
</p><!--l. 256--><p class="indent" >   <img 
src="machine_learning-2-.png" alt="PIC"  
width="286" height="170"  />
</p><!--l. 258--><p class="indent" >    
</p><!--l. 260--><p class="indent" >    
</p><!--l. 262--><p class="indent" >    
</p><!--l. 264--><p class="indent" >   For the output layer (<span 
class="cmmi-10">L</span><sup><span 
class="cmr-7">th</span></sup> layer), <span 
class="cmmi-10">δ</span><sub><span 
class="cmmi-7">j</span></sub><sup><span 
class="cmmi-7">l</span></sup> deﬁned in Eq. (<a 
href="#x1-6001r16">16<!--tex4ht:ref: 22-3-11-p1 --></a>) is written as
</p>
   <table 
class="equation"><tr><td><a 
 id="x1-6004r19"></a>
   <center class="math-display" >
<img 
src="machine_learning18x.png" alt="     ∂Cx   ∂Cx ∂aLj
δLj = ∂zL-= ∂aL-∂zL-.
       j     j   j
" class="math-display"  /></center></td><td class="equation-label">(19)</td></tr></table>
<!--l. 269--><p class="nopar" >
The dependence of <span 
class="cmmi-10">C</span><sub><span 
class="cmmi-7">x</span></sub> on <span 
class="cmmi-10">a</span><sub><span 
class="cmmi-7">j</span></sub><sup><span 
class="cmmi-7">L</span></sup> is explicitly given by Eq. (<a 
href="#x1-5001r12">12<!--tex4ht:ref: 22-3-11-a6 --></a>), from which the above expression for <span 
class="cmmi-10">δ</span><sub><span 
class="cmmi-7">j</span></sub><sup><span 
class="cmmi-7">L</span></sup> is
written as
</p>
   <table 
class="equation"><tr><td><a 
 id="x1-6005r20"></a>

   <center class="math-display" >
<img 
src="machine_learning19x.png" alt=" L     L        ′  L
δj = (aj − yj(x))σ (zj ).
" class="math-display"  /></center></td><td class="equation-label">(20)</td></tr></table>
<!--l. 275--><p class="nopar" >
Therefore <span 
class="cmmi-10">δ</span><sub><span 
class="cmmi-7">j</span></sub><sup><span 
class="cmmi-7">L</span></sup> is easy to compute.
</p><!--l. 278--><p class="indent" >   Backpropagation is a way of computing <span 
class="cmmi-10">δ</span><sub><span 
class="cmmi-7">j</span></sub><sup><span 
class="cmmi-7">l</span></sup> for every layer using recurrence relations: the relation
between <span 
class="cmmi-10">δ</span><sup><span 
class="cmmi-7">l</span></sup> and <span 
class="cmmi-10">δ</span><sup><span 
class="cmmi-7">l</span><span 
class="cmr-7">+1</span></sup>. Noting how the error is propagating through the network, we know the following
identity:
</p>
   <table 
class="equation"><tr><td><a 
 id="x1-6006r21"></a>
   <center class="math-display" >
<img 
src="machine_learning20x.png" alt="∂Cx- l   ∑  -∂Cx-  l+1
∂zldzJ =    ∂zl+1dzj ,
  J       j   j
" class="math-display"  /></center></td><td class="equation-label">(21)</td></tr></table>
<!--l. 285--><p class="nopar" >
with
</p>
   <table 
class="equation"><tr><td><a 
 id="x1-6007r22"></a>
   <center class="math-display" >
<img 
src="machine_learning21x.png" alt="  l+1    l+1  l
dzj   = wjJ d(aJ),
" class="math-display"  /></center></td><td class="equation-label">(22)</td></tr></table>
<!--l. 289--><p class="nopar" >
i.e.,
</p>
   <table 
class="equation"><tr><td><a 
 id="x1-6008r23"></a>

   <center class="math-display" >
<img 
src="machine_learning22x.png" alt="  l+1    l+1 ′ l   l
dzj  = wjJ σ(zJ)dzJ.
" class="math-display"  /></center></td><td class="equation-label">(23)</td></tr></table>
<!--l. 293--><p class="nopar" >
Therefore
</p>
   <table 
class="equation"><tr><td><a 
 id="x1-6009r24"></a>
   <center class="math-display" >
<img 
src="machine_learning23x.png" alt="∂Cx-  ∑  -∂Cx-  l+1 ′ l
∂zl =    ∂zl+1wjJ σ (zJ).
  J    j   j
" class="math-display"  /></center></td><td class="equation-label">(24)</td></tr></table>
<!--l. 298--><p class="nopar" >
i.e.,
</p>
   <table 
class="equation"><tr><td><a 
 id="x1-6010r25"></a>
   <center class="math-display" >
<img 
src="machine_learning24x.png" alt=" l   ∑   l+1  l+1 ′  l
δJ =    δj w jJ σ (zJ).
      j
" class="math-display"  /></center></td><td class="equation-label">(25)</td></tr></table>
<!--l. 303--><p class="nopar" >
Equation (<a 
href="#x1-6010r25">25<!--tex4ht:ref: 22-3-14-a1 --></a>) gives the recurrence relations of computing <span 
class="cmmi-10">δ</span><sup><span 
class="cmmi-7">l</span></sup> from <span 
class="cmmi-10">δ</span><sup><span 
class="cmmi-7">l</span><span 
class="cmr-7">+1</span></sup>. This is called the
backpropagation algorithm. Eq. (<a 
href="#x1-6010r25">25<!--tex4ht:ref: 22-3-14-a1 --></a>) can be written in the matrix form:
</p>
   <table 
class="equation"><tr><td><a 
 id="x1-6011r26"></a>

   <center class="math-display" >
<img 
src="machine_learning25x.png" alt=" l     l+1 T l+1    ′ l
δ = ((w  ) δ   )⊙ σ (z ),
" class="math-display"  /></center></td><td class="equation-label">(26)</td></tr></table>
<!--l. 309--><p class="nopar" >
where <span 
class="cmmi-10">T </span>stands for matrix transpose, <span 
class="cmsy-10">⊙ </span>is the element-wise product.
</p><!--l. 312--><p class="noindent" >
</p>
   <h3 class="sectionHead"><span class="titlemark">3   </span> <a 
 id="x1-70003"></a>misc</h3>
<!--l. 314--><p class="noindent" >
</p>
   <h3 class="sectionHead"><span class="titlemark">4   </span> <a 
 id="x1-80004"></a>Least square</h3>
<!--l. 316--><p class="noindent" >In the least square method, the loss function is deﬁned as
</p>
   <table 
class="equation"><tr><td><a 
 id="x1-8001r27"></a>
   <center class="math-display" >
<img 
src="machine_learning26x.png" alt="    n
L = ∑  |ˆy(x )− y |2,
    i=1    i    i
" class="math-display"  /></center></td><td class="equation-label">(27)</td></tr></table>
<!--l. 319--><p class="nopar" >
where <img 
src="machine_learning27x.png" alt="ˆy"  class="circ"  />(<span 
class="cmbx-10">x</span><sub><span 
class="cmmi-7">i</span></sub>) is the output of the model for the input <span 
class="cmbx-10">x</span><sub><span 
class="cmmi-7">i</span></sub>, <span 
class="cmmi-10">n </span>is the number of data points.
</p><!--l. 323--><p class="indent" >   In the most general case, each data point considers of multiple independent variables and multiple
dependent variables (<span 
class="cmbx-10">x</span><span 
class="cmmi-10">,</span><span 
class="cmbx-10">y</span>). In simple cases, each data point has one independent variable and one
dependent variable. For example, a data set consists of <span 
class="cmti-10">n</span> data-points (<span 
class="cmmi-10">x</span><sub><span 
class="cmmi-7">i</span></sub><span 
class="cmmi-10">,y</span><sub><span 
class="cmmi-7">i</span></sub>), <span 
class="cmti-10">i</span> = 1, …, <span 
class="cmti-10">n</span>, where <span 
class="cmmi-10">x</span><sub><span 
class="cmmi-7">i</span></sub> is an
independent variable and <span 
class="cmmi-10">y</span><sub><span 
class="cmmi-7">i</span></sub> is a dependent variable whose value is found by observation. The model
function has the form <span 
class="cmmi-10">y</span>(<span 
class="cmmi-10">x</span>) = <span 
class="cmmi-10">f</span>(<span 
class="cmmi-10">x,β</span>), where <span 
class="cmti-10">m</span> adjustable parameters are held in the vector <span 
class="cmmi-10">β</span>. A least
squar model is called linear if the model comprises a linear combination of the parameters,
i.e.,
</p>
   <table 
class="equation"><tr><td><a 
 id="x1-8002r28"></a>

   <center class="math-display" >
<img 
src="machine_learning28x.png" alt="         m∑
f (x,β) =    βjφj(x ),
         j=1
" class="math-display"  /></center></td><td class="equation-label">(28)</td></tr></table>
<!--l. 335--><p class="nopar" >
where <span 
class="cmmi-10">φ</span><sub><span 
class="cmmi-7">j</span></sub>(<span 
class="cmmi-10">x</span>) are basis functions chosen. Letting <span 
class="cmmi-10">X</span><sub><span 
class="cmmi-7">ij</span></sub> = <span 
class="cmmi-10">φ</span><sub><span 
class="cmmi-7">j</span></sub>(<span 
class="cmmi-10">x</span><sub><span 
class="cmmi-7">i</span></sub>), then the model prediction for input <span 
class="cmmi-10">x</span><sub><span 
class="cmmi-7">i</span></sub>
can be written as
</p>
   <table 
class="equation"><tr><td><a 
 id="x1-8003r29"></a>
   <center class="math-display" >
<img 
src="machine_learning29x.png" alt="             ∑m
fi ≡ f(xi,β ) =  Xijβj.
             j=1
" class="math-display"  /></center></td><td class="equation-label">(29)</td></tr></table>
<!--l. 340--><p class="nopar" >
For <span 
class="cmmi-10">n </span>data points, the above can be written in matrix form:
</p>
   <table 
class="equation"><tr><td><a 
 id="x1-8004r30"></a>
   <center class="math-display" >
<img 
src="machine_learning30x.png" alt="f = Xβ,
" class="math-display"  /></center></td><td class="equation-label">(30)</td></tr></table>
<!--l. 344--><p class="nopar" >
where <span 
class="cmbx-10">f </span>= (<span 
class="cmmi-10">f</span><sub><span 
class="cmr-7">1</span></sub><span 
class="cmmi-10">,</span><span 
class="cmmi-10">…</span><span 
class="cmmi-10">f</span><sub><span 
class="cmmi-7">n</span></sub>)<sup><span 
class="cmmi-7">T</span></sup>.
</p><!--l. 347--><p class="indent" >   For linear least-square ﬁtting, we can solve the “normal equation” to get the ﬁtting coeﬃcients.
Alternatively, one can use iterative methods, e.g., the gradient descent method, to minimize the
mean square error over the coeﬃcients. The following is a complete example in Python:
</p><div class="alltt">

<div class="obeylines-v">            <span 
class="cmtt-10">import</span><span 
class="cmtt-10"> numpy</span><span 
class="cmtt-10"> as</span><span 
class="cmtt-10"> np</span>
<br /><span 
class="cmtt-10">import</span><span 
class="cmtt-10"> matplotlib.pyplot</span><span 
class="cmtt-10"> as</span><span 
class="cmtt-10"> plt</span>
<br /><span 
class="cmtt-10">class</span><span 
class="cmtt-10"> Linear_Regression:</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> def</span><span 
class="cmtt-10"> __init__(self):</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> self.b</span><span 
class="cmtt-10"> =</span><span 
class="cmtt-10"> [0,</span><span 
class="cmtt-10"> 0]</span>
<br />
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> def</span><span 
class="cmtt-10"> predict(self,</span><span 
class="cmtt-10"> X):</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> Y_pred</span><span 
class="cmtt-10"> =</span><span 
class="cmtt-10"> self.b[0]</span><span 
class="cmtt-10"> +</span><span 
class="cmtt-10"> self.b[1]*X</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> return</span><span 
class="cmtt-10"> Y_pred</span>
<br />
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> def</span><span 
class="cmtt-10"> update_coeffs(self,</span><span 
class="cmtt-10"> X,</span><span 
class="cmtt-10"> Y,</span><span 
class="cmtt-10"> learning_rate):</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> Y_pred</span><span 
class="cmtt-10"> =</span><span 
class="cmtt-10"> self.predict(X)</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> m</span><span 
class="cmtt-10"> =</span><span 
class="cmtt-10"> len(Y)</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> self.b[0]</span><span 
class="cmtt-10"> =</span><span 
class="cmtt-10"> self.b[0]</span><span 
class="cmtt-10"> -</span><span 
class="cmtt-10"> (learning_rate</span><span 
class="cmtt-10"> *</span><span 
class="cmtt-10"> ((1/m)</span><span 
class="cmtt-10"> *</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> np.sum(Y_pred</span><span 
class="cmtt-10"> -</span><span 
class="cmtt-10"> Y)))</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> self.b[1]</span><span 
class="cmtt-10"> =</span><span 
class="cmtt-10"> self.b[1]</span><span 
class="cmtt-10"> -</span><span 
class="cmtt-10"> (learning_rate</span><span 
class="cmtt-10"> *</span><span 
class="cmtt-10"> ((1/m)</span><span 
class="cmtt-10"> *</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> np.sum((Y_pred</span><span 
class="cmtt-10"> -</span><span 
class="cmtt-10"> Y)</span><span 
class="cmtt-10"> *</span><span 
class="cmtt-10"> X)))</span>
<br />
<br />
<br /><span 
class="cmtt-10">regressor</span><span 
class="cmtt-10"> =</span><span 
class="cmtt-10"> Linear_Regression()</span>
<br /><span 
class="cmtt-10">Nd=11</span>
<br /><span 
class="cmtt-10">X</span><span 
class="cmtt-10"> =</span><span 
class="cmtt-10"> np.array([i</span><span 
class="cmtt-10"> for</span><span 
class="cmtt-10"> i</span><span 
class="cmtt-10"> in</span><span 
class="cmtt-10"> range(Nd)])</span>
<br /><span 
class="cmtt-10">Y</span><span 
class="cmtt-10"> =</span><span 
class="cmtt-10"> np.array([2*i</span><span 
class="cmtt-10"> for</span><span 
class="cmtt-10"> i</span><span 
class="cmtt-10"> in</span><span 
class="cmtt-10"> range(Nd)])</span><span 
class="cmtt-10"> +</span><span 
class="cmtt-10"> np.random.uniform(high=5.0,size=Nd)</span>
<br /><span 
class="cmtt-10">fig,</span><span 
class="cmtt-10"> ax</span><span 
class="cmtt-10"> =</span><span 
class="cmtt-10"> plt.subplots()</span>
<br /><span 
class="cmtt-10">ax.plot(X,Y,</span><span 
class="cmtt-10"> ’k.’,label=’data’)</span>
<br /><span 
class="cmtt-10">Y_pred</span><span 
class="cmtt-10"> =</span><span 
class="cmtt-10"> regressor.predict(X)</span>
<br /><span 
class="cmtt-10">ax.plot(X,</span><span 
class="cmtt-10"> Y_pred,</span><span 
class="cmtt-10"> label=’Initial</span><span 
class="cmtt-10"> fit</span><span 
class="cmtt-10"> line’)</span>
<br />
<br /><span 
class="cmtt-10">learning_rate</span><span 
class="cmtt-10"> =</span><span 
class="cmtt-10"> 0.01</span>
<br /><span 
class="cmtt-10">i</span><span 
class="cmtt-10"> =</span><span 
class="cmtt-10"> 0</span>
<br /><span 
class="cmtt-10">while</span><span 
class="cmtt-10"> i&#x003C;100:</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> regressor.update_coeffs(X,Y,learning_rate)</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> i</span><span 
class="cmtt-10"> =</span><span 
class="cmtt-10"> i+1</span>
<br />
<br /><span 
class="cmtt-10">Y_pred</span><span 
class="cmtt-10"> =</span><span 
class="cmtt-10"> regressor.predict(X)</span>
<br /><span 
class="cmtt-10">ax.plot(X,</span><span 
class="cmtt-10"> Y_pred,</span><span 
class="cmtt-10"> ’b-’,label=’Final</span><span 
class="cmtt-10"> Fit</span><span 
class="cmtt-10"> Line’)</span>
<br /><span 
class="cmtt-10">ax.legend()</span>
<br /><span 
class="cmtt-10">plt.show()</span>
</div>
</div>
The loss function is deﬁned by the mean square error, which  is not directly used in the above code.
Only the partial derivatives of the loss function is directly used.
<!--l. 395--><p class="noindent" >
</p>
   <h3 class="sectionHead"><span class="titlemark">5   </span> <a 
 id="x1-90005"></a>Logistic regression for binary classiﬁcation</h3>

<!--l. 397--><p class="noindent" >Hypothesis function (the model): Denote the output of the model by <span 
class="cmmi-10">ŷ</span>, which is given by
</p>
   <table 
class="equation"><tr><td><a 
 id="x1-9001r31"></a>
   <center class="math-display" >
<img 
src="machine_learning31x.png" alt="ˆy = σ(z),
" class="math-display"  /></center></td><td class="equation-label">(31)</td></tr></table>
<!--l. 401--><p class="nopar" >
where <span 
class="cmmi-10">z </span>= <span 
class="cmmi-10">w </span><span 
class="cmsy-10">⋅ </span><span 
class="cmmi-10">x </span>+ <span 
class="cmmi-10">b </span>and <span 
class="cmmi-10">σ </span>is the sigmoid function given by Eq. (<a 
href="#x1-3005r4">4<!--tex4ht:ref: 23-7-28-p1 --></a>). The model is nonlinear in the
unknowns <span 
class="cmmi-10">w </span>and <span 
class="cmmi-10">b</span>.
</p><!--l. 405--><p class="indent" >   The loss function is chosen as
</p>
   <table 
class="equation"><tr><td><a 
 id="x1-9002r32"></a>
   <center class="math-display" >
<img 
src="machine_learning32x.png" alt="        m
L = − 1-∑ [y log(ˆy )+ (1 − y)log(1− ˆy)],
      m i=1  i    i        i        i
" class="math-display"  /></center></td><td class="equation-label">(32)</td></tr></table>
<!--l. 409--><p class="nopar" >
where <span 
class="cmmi-10">y</span><sub><span 
class="cmmi-7">i</span></sub> is the correct answer of the ith training example (<span 
class="cmmi-10">y</span><sub><span 
class="cmmi-7">i</span></sub> can take only two values, 0 or 1). The
value of <img 
src="machine_learning33x.png" alt="y^i"  class="widehat"  /> is interpreted as the probability of <span 
class="cmmi-10">y </span>being 1.
</p><!--l. 414--><p class="indent" >   Because the model function is nonlinear and the loss function is complicated, there is usually no
closed-form solution that minimizes the loss function. Iterative methods, such as gradient descent, are
needed to solve for <span 
class="cmmi-10">w </span>and <span 
class="cmmi-10">b</span>. The partial derivatives needed in the gradient desent method can be
written as </p><div class="eqnarray">
   <center class="math-display" >
<img 
src="machine_learning34x.png" alt="               [                              ]
∂L- =   −-1 ∑   y 1σ′(z)x  − (1 − y)--1---σ′(z)x
∂w       m   i   iˆyi      i      i (1− ˆyi)     i
          1 ∑  {[  1            1   ]      }
    =   −--      yi--− (1− yi)-------σ ′(z)xi
         m   i {[  ˆyi    ]    (1 −}ˆyi)
         -1 ∑    --yi −y^i   ′
    =   −m       ˆyi(1− ˆyi) σ (z)xi
            ∑i {[        ]          }
    =   −-1      --yi −y^i  σ(1− σ)xi
         m   i   ˆyi(1− ˆyi)
          1 ∑  {[  yi −y^i ]         }
    =   −m-      ˆy-(1−-ˆy-) ˆyi(1− yˆi)xi
             i    i     i
    =   −-1 ∑ [(yi − ^yi)xi]                                  (33)
         m   i
" class="math-display"  /></center>
</div>Using
   <table 
class="equation"><tr><td><a 
 id="x1-9004r34"></a>
   <center class="math-display" >
<img 
src="machine_learning35x.png" alt="dσ-  -----1-------          2
dz = (1 + exp(− z))2 exp(− z) = σ exp(− z) = σ(1− σ)
" class="math-display"  /></center></td><td class="equation-label">(34)</td></tr></table>
<!--l. 440--><p class="nopar" >
The above formula is simpliﬁed as
</p>
   <table 
class="equation"><tr><td><a 
 id="x1-9005r35"></a>
   <center class="math-display" >
<img 
src="machine_learning36x.png" alt="              { [      ]         }
∂L-  =  − 1-∑    -yi−y^i- σ(1− σ )xi
∂w        m1 ∑ i{ [ˆyi(y1i−−y ˆy^ii)]         }
     =  − m-∑ i  ˆyi(1− ˆyi) ˆyi(1− ˆyi)xi
     =  − 1m-  i[(yi − y^i)xi]

" class="math-display"  /></center></td><td class="equation-label">(35)</td></tr></table>
<!--l. 452--><p class="nopar" >
Similary, we obtain
</p>
   <table 
class="equation"><tr><td><a 
 id="x1-9006r36"></a>
   <center class="math-display" >
<img 
src="machine_learning37x.png" alt="∂L-= − 1-∑  (y − ^y).
∂w     m  i   i   i
" class="math-display"  /></center></td><td class="equation-label">(36)</td></tr></table>
<!--l. 456--><p class="nopar" >
</p><!--l. 458--><p class="noindent" >
</p>
   <h4 class="subsectionHead"><span class="titlemark">5.1   </span> <a 
 id="x1-100005.1"></a>Automatic diﬀerentiation</h4>
<!--l. 460--><p class="noindent" >Forward: </p><div class="alltt">
<div class="obeylines-v">            <span 
class="cmtt-10">class</span><span 
class="cmtt-10"> Expression:</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> def</span><span 
class="cmtt-10"> __add__(self,</span><span 
class="cmtt-10"> other):</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> return</span><span 
class="cmtt-10"> Plus(self,</span><span 
class="cmtt-10"> other)</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> def</span><span 
class="cmtt-10"> __mul__(self,</span><span 
class="cmtt-10"> other):</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> return</span><span 
class="cmtt-10"> Multiply(self,other)</span>
<br /><span 
class="cmtt-10">class</span><span 
class="cmtt-10"> Variable(Expression):</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> def</span><span 
class="cmtt-10"> __init__(self,value):</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> self.value</span><span 
class="cmtt-10"> =</span><span 
class="cmtt-10"> value</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> def</span><span 
class="cmtt-10"> evaluate(self):</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> return</span><span 
class="cmtt-10"> self.value</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> def</span><span 
class="cmtt-10"> derive(self,</span><span 
class="cmtt-10"> v):</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> return</span><span 
class="cmtt-10"> 1</span><span 
class="cmtt-10"> if</span><span 
class="cmtt-10"> self</span><span 
class="cmtt-10"> ==</span><span 
class="cmtt-10"> v</span><span 
class="cmtt-10"> else</span><span 
class="cmtt-10"> 0</span>
<br />
<br /><span 
class="cmtt-10">class</span><span 
class="cmtt-10"> Plus(Expression):</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> def</span><span 
class="cmtt-10"> __init__(self,exp1,exp2):</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> self.a</span><span 
class="cmtt-10"> =</span><span 
class="cmtt-10"> exp1</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> self.b</span><span 
class="cmtt-10"> =</span><span 
class="cmtt-10"> exp2</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> def</span><span 
class="cmtt-10"> evaluate(self):</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> return</span><span 
class="cmtt-10"> self.a.evaluate()</span><span 
class="cmtt-10"> +</span><span 
class="cmtt-10"> self.b.evaluate()</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> def</span><span 
class="cmtt-10"> derive(self,exp):</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> return</span><span 
class="cmtt-10"> self.a.derive(exp)</span><span 
class="cmtt-10"> +</span><span 
class="cmtt-10"> self.b.derive(exp)</span>

<br />
<br /><span 
class="cmtt-10">class</span><span 
class="cmtt-10"> Multiply(Expression):</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> def</span><span 
class="cmtt-10"> __init__(self,exp1,exp2):</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> self.a</span><span 
class="cmtt-10"> =</span><span 
class="cmtt-10"> exp1</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> self.b</span><span 
class="cmtt-10"> =</span><span 
class="cmtt-10"> exp2</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> def</span><span 
class="cmtt-10"> evaluate(self):</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> return</span><span 
class="cmtt-10"> self.a.evaluate()</span><span 
class="cmtt-10"> *</span><span 
class="cmtt-10"> self.b.evaluate()</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> def</span><span 
class="cmtt-10"> derive(self,exp):</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> return</span><span 
class="cmtt-10"> (self.a.derive(exp)*self.b.evaluate()</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> +self.b.derive(exp)*self.a.evaluate())</span>
<br />
<br /><span 
class="cmtt-10">#</span><span 
class="cmtt-10"> Example:</span><span 
class="cmtt-10"> derivatives</span><span 
class="cmtt-10"> of</span><span 
class="cmtt-10"> z</span><span 
class="cmtt-10"> =</span><span 
class="cmtt-10"> x</span><span 
class="cmtt-10"> *</span><span 
class="cmtt-10"> (x</span><span 
class="cmtt-10"> +</span><span 
class="cmtt-10"> y)</span><span 
class="cmtt-10"> +</span><span 
class="cmtt-10"> y</span><span 
class="cmtt-10"> *</span><span 
class="cmtt-10"> y</span><span 
class="cmtt-10"> at</span><span 
class="cmtt-10"> (x,</span><span 
class="cmtt-10"> y)</span><span 
class="cmtt-10"> =</span><span 
class="cmtt-10"> (2,</span><span 
class="cmtt-10"> 3)</span>
<br /><span 
class="cmtt-10">x</span><span 
class="cmtt-10"> =</span><span 
class="cmtt-10"> Variable(2)</span>
<br /><span 
class="cmtt-10">y</span><span 
class="cmtt-10"> =</span><span 
class="cmtt-10"> Variable(3)</span>
<br /><span 
class="cmtt-10">z</span><span 
class="cmtt-10"> =</span><span 
class="cmtt-10"> x</span><span 
class="cmtt-10"> *</span><span 
class="cmtt-10"> (x</span><span 
class="cmtt-10"> +</span><span 
class="cmtt-10"> y)</span><span 
class="cmtt-10"> +</span><span 
class="cmtt-10"> y</span><span 
class="cmtt-10"> *</span><span 
class="cmtt-10"> y</span>
<br /><span 
class="cmtt-10">print(z.evaluate())</span><span 
class="cmtt-10"> #</span><span 
class="cmtt-10"> z,</span><span 
class="cmtt-10"> Output:</span><span 
class="cmtt-10"> 19</span>
<br /><span 
class="cmtt-10">print(z.derive(x))</span><span 
class="cmtt-10"> #</span><span 
class="cmtt-10"> dz/dx,</span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> Output:</span><span 
class="cmtt-10"> 7</span>
<br /><span 
class="cmtt-10">print(z.derive(y))</span><span 
class="cmtt-10"> #</span><span 
class="cmtt-10"> dz/dy,</span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> Output:</span><span 
class="cmtt-10"> 8</span>
</div>
</div>
This simple example illustrates many concepts:
<!--l. 504--><p class="indent" >   * Class
</p><!--l. 506--><p class="indent" >   * Operator overloading
</p><!--l. 508--><p class="indent" >   * Subclass, inheritance
</p><!--l. 510--><p class="indent" >   * Polymorphism
</p><!--l. 512--><p class="indent" >   * Recursion
</p><!--l. 514--><p class="indent" >    
</p><!--l. 516--><p class="indent" >   The following is the backword (or reverse) method. This code is a little harder to understand than
the the forward method. </p><div class="alltt">
<div class="obeylines-v">            <span 
class="cmtt-10">class</span><span 
class="cmtt-10"> Expression:</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> def</span><span 
class="cmtt-10"> __add__(self,</span><span 
class="cmtt-10"> other):</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> return</span><span 
class="cmtt-10"> Plus(self,</span><span 
class="cmtt-10"> other)</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> def</span><span 
class="cmtt-10"> __mul__(self,</span><span 
class="cmtt-10"> other):</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> return</span><span 
class="cmtt-10"> Multiply(self,</span><span 
class="cmtt-10"> other)</span>
<br />
<br /><span 
class="cmtt-10">class</span><span 
class="cmtt-10"> Variable(Expression):</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> def</span><span 
class="cmtt-10"> __init__(self,</span><span 
class="cmtt-10"> value):</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> self.value</span><span 
class="cmtt-10"> =</span><span 
class="cmtt-10"> value</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> self.partial</span><span 
class="cmtt-10"> =</span><span 
class="cmtt-10"> 0</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> def</span><span 
class="cmtt-10"> evaluate(self):</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> pass</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> def</span><span 
class="cmtt-10"> derive(self,</span><span 
class="cmtt-10"> seed):</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> self.partial</span><span 
class="cmtt-10"> +=</span><span 
class="cmtt-10"> seed</span>
<br />
<br /><span 
class="cmtt-10">class</span><span 
class="cmtt-10"> Plus(Expression):</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> def</span><span 
class="cmtt-10"> __init__(self,</span><span 
class="cmtt-10"> exp1,</span><span 
class="cmtt-10"> exp2):</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> self.a</span><span 
class="cmtt-10"> =</span><span 
class="cmtt-10"> exp1</span>

<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> self.b</span><span 
class="cmtt-10"> =</span><span 
class="cmtt-10"> exp2</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> self.value</span><span 
class="cmtt-10"> =</span><span 
class="cmtt-10"> None</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> def</span><span 
class="cmtt-10"> evaluate(self):</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> self.a.evaluate()</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> self.b.evaluate()</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> self.value</span><span 
class="cmtt-10"> =</span><span 
class="cmtt-10"> self.a.value</span><span 
class="cmtt-10"> +</span><span 
class="cmtt-10"> self.b.value</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> def</span><span 
class="cmtt-10"> derive(self,</span><span 
class="cmtt-10"> seed):</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> self.a.derive(seed)</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> self.b.derive(seed)</span>
<br />
<br /><span 
class="cmtt-10">class</span><span 
class="cmtt-10"> Multiply(Expression):</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> def</span><span 
class="cmtt-10"> __init__(self,</span><span 
class="cmtt-10"> exp1,</span><span 
class="cmtt-10"> exp2):</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> self.a</span><span 
class="cmtt-10"> =</span><span 
class="cmtt-10"> exp1</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> self.b</span><span 
class="cmtt-10"> =</span><span 
class="cmtt-10"> exp2</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> self.value</span><span 
class="cmtt-10"> =</span><span 
class="cmtt-10"> None</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> def</span><span 
class="cmtt-10"> evaluate(self):</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> self.a.evaluate()</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> self.b.evaluate()</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> self.value</span><span 
class="cmtt-10"> =</span><span 
class="cmtt-10"> self.a.value</span><span 
class="cmtt-10"> *</span><span 
class="cmtt-10"> self.b.value</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> def</span><span 
class="cmtt-10"> derive(self,</span><span 
class="cmtt-10"> seed):</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> self.a.derive(self.b.value</span><span 
class="cmtt-10"> *</span><span 
class="cmtt-10"> seed)</span>
<br /><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> self.b.derive(self.a.value</span><span 
class="cmtt-10"> *</span><span 
class="cmtt-10"> seed)</span>
<br />
<br /><span 
class="cmtt-10">#</span><span 
class="cmtt-10"> Example:</span><span 
class="cmtt-10"> Finding</span><span 
class="cmtt-10"> the</span><span 
class="cmtt-10"> partials</span><span 
class="cmtt-10"> of</span><span 
class="cmtt-10"> z</span><span 
class="cmtt-10"> =</span><span 
class="cmtt-10"> x</span><span 
class="cmtt-10"> *</span><span 
class="cmtt-10"> (x</span><span 
class="cmtt-10"> +</span><span 
class="cmtt-10"> y)</span><span 
class="cmtt-10"> +</span><span 
class="cmtt-10"> y</span><span 
class="cmtt-10"> *</span><span 
class="cmtt-10"> y</span><span 
class="cmtt-10"> at</span><span 
class="cmtt-10"> (x,</span><span 
class="cmtt-10"> y)</span><span 
class="cmtt-10"> =</span><span 
class="cmtt-10"> (2,</span><span 
class="cmtt-10"> 3)</span>
<br /><span 
class="cmtt-10">x</span><span 
class="cmtt-10"> =</span><span 
class="cmtt-10"> Variable(2)</span>
<br /><span 
class="cmtt-10">y</span><span 
class="cmtt-10"> =</span><span 
class="cmtt-10"> Variable(3)</span>
<br /><span 
class="cmtt-10">z</span><span 
class="cmtt-10"> =</span><span 
class="cmtt-10"> x</span><span 
class="cmtt-10"> *</span><span 
class="cmtt-10"> (x</span><span 
class="cmtt-10"> +</span><span 
class="cmtt-10"> y)</span><span 
class="cmtt-10"> +</span><span 
class="cmtt-10"> y</span><span 
class="cmtt-10"> *</span><span 
class="cmtt-10"> y</span>
<br /><span 
class="cmtt-10">z.evaluate();</span><span 
class="cmtt-10"> print(&#x0022;z</span><span 
class="cmtt-10"> =&#x0022;,</span><span 
class="cmtt-10"> z.value)</span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> #</span><span 
class="cmtt-10"> Output</span><span 
class="cmtt-10"> 19</span>
<br /><span 
class="cmtt-10">z.derive(1)</span>
<br /><span 
class="cmtt-10">print(x.partial)</span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> #</span><span 
class="cmtt-10"> dz/dx</span><span 
class="cmtt-10"> Output:</span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> 7</span>
<br /><span 
class="cmtt-10">print(y.partial)</span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> #</span><span 
class="cmtt-10"> dz/dy</span><span 
class="cmtt-10"> Output:</span><span 
class="cmtt-10"> </span><span 
class="cmtt-10"> 8</span>
</div>
</div>
   <h3 class="likesectionHead"><a 
 id="x1-110005.1"></a>References</h3>
<!--l. 569--><p class="noindent" >
   </p><div class="thebibliography">
   <p class="bibitem" ><span class="biblabel">
 [1]<span class="bibsp">   </span></span><a 
 id="Xnielsen2015neural"></a>Michael A Nielsen. <span 
class="cmti-10">Neural networks and deep learning</span>, volume 25. Determination Press:
   http://neuralnetworksanddeeplearning.com/, 2015.</p></div>
<!--l. 575--><p class="indent" >    
</p><!--l. 577--><p class="indent" >    
</p>
    
</body></html> 



