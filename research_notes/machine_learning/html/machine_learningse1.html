<?xml version="1.0" encoding="utf-8" ?> 
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" 
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">  
<!--http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd-->  
<html xmlns="http://www.w3.org/1999/xhtml"  
> 
<head><title>Introduction</title> 
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" /> 
<meta name="generator" content="TeX4ht (http://www.tug.org/tex4ht/)" /> 
<meta name="originator" content="TeX4ht (http://www.tug.org/tex4ht/)" /> 
<!-- xhtml,2,charset=utf-8,html --> 
<meta name="src" content="machine_learning.tex" /> 
<link rel="stylesheet" type="text/css" href="machine_learning.css" /> 
</head><body 
>
   <!--l. 25--><div class="crosslinks"><p class="noindent">[<a 
href="machine_learningse2.html" >next</a>] [<a 
href="#tailmachine_learningse1.html">tail</a>] [<a 
href="machine_learning.html#machine_learningse1.html" >up</a>] </p></div>
   <h3 class="sectionHead"><span class="titlemark">1   </span> <a 
 id="x2-10001"></a>Introduction</h3>
<!--l. 27--><p class="noindent" >Artiﬁcial intelligence (AI) research has tried many diﬀerent approaches since its founding. In the ﬁrst
decades of the 21st century, the AI research is dominated by highly mathematical statistical machine
learning (ML), which has proved highly successful, helping to solve many challenging problems in real
life.
</p><!--l. 33--><p class="indent" >   Many problems in AI can be solved theoretically by searching through many possible solutions:
Reasoning can be reduced to performing a search. Simple exhaustive searches are rarely suﬃcient for
most real-world problems. The solution, for many problems, is to use ”heuristics” or ”rules of thumb”
that prioritize choices in favor of those more likely to reach a goal. A very diﬀerent kind of search came
to prominence in the 1990s, based on the mathematical theory of optimization. Modern machine
learning is based on these methods. Instead, of using detailed explanations to guide the search, it uses
a combination of<span class="cite">[<a 
href="machine_learningli1.html#Xnielsen2015neural">1</a>]</span>: (a) general architectures; (b) trying trillions of possibilities, guided
by simple ideas (like gradient descent) for improvement; and (c) the ability to recognize
progress.

</p><!--l. 45--><p class="indent" >   I am interested in applying machine learning to problems in computational physics problems that
traditional numerical methods can not easily handle either because of its computational costs being too
high or its traditional algorithms are too complicated to easily implement.
</p><!--l. 50--><p class="indent" >   Enrico Fermi once criticized the complexity of a model (that contains many free parameters) by
quoting Johnny von Neumann “With four parameters I can ﬁt an elephant, and with ﬁve I can make
him wiggle his trunk”.
</p><!--l. 54--><p class="indent" >   What Fermi implies is that it is easy to ﬁt existing data and what is important is to have a model
with predicting capability (ﬁtting data not seen yet). The artiﬁcial neural network method tackles this
diﬃculty by increasing the number of free parameters to millions, with the hope of obtaining predicting
capability.

</p>
   <!--l. 60--><div class="crosslinks"><p class="noindent">[<a 
href="machine_learningse2.html" >next</a>] [<a 
href="machine_learningse1.html" >front</a>] [<a 
href="machine_learning.html#machine_learningse1.html" >up</a>] </p></div>
<!--l. 60--><p class="indent" >   <a 
 id="tailmachine_learningse1.html"></a> </p> 
</body></html> 
